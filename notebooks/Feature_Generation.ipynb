{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(name, data):\n",
    "    \"\"\" save object to disk for caching \"\"\"\n",
    "    import pickle\n",
    "\n",
    "    with open('../data/processed/'+name, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "\n",
    "def load(name):\n",
    "    import pickle\n",
    "    \n",
    "    with open('../data/processed/'+name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_stopword_matrix():\n",
    "    words = load('text_vectorizer').get_feature_names()\n",
    "    \n",
    "    matrix = sparse.eye(len(words), format='dok')\n",
    "    \n",
    "    for word_id, word in enumerate(words):\n",
    "        if word in ENGLISH_STOP_WORDS:\n",
    "            matrix[word_id, word_id] = 0\n",
    "        \n",
    "    return matrix.tocsr()\n",
    "\n",
    "def make_stem_matrix():\n",
    "    words = load('text_vectorizer').get_feature_names()\n",
    "    \n",
    "    # stem all words\n",
    "    stems = defaultdict(list)\n",
    "    \n",
    "    for word_id, word in enumerate(words):\n",
    "        stems[stemmer.stem(word)].append(word_id)\n",
    "        \n",
    "    # make matrix\n",
    "    matrix = dok_matrix((len(words), len(stems)))\n",
    "    \n",
    "    for stem_id, s in enumerate(stems):\n",
    "        for word_id in stems[s]:\n",
    "            matrix[word_id, stem_id] = 1.\n",
    "            \n",
    "    return matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save('stopword_matrix', make_stopword_matrix())\n",
    "save('stem_matrix', make_stem_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_featureset(name, stem=True, tf_idf=True, stopwords=False, norm='l2',\n",
    "                   use_idf=1, smooth_idf=1, sublinear_tf=1, binary=False):\n",
    "\n",
    "    data = parse(name)\n",
    "\n",
    "    if stopwords:\n",
    "        data = data * load('stopword_matrix')\n",
    "    if stem:\n",
    "        data = data * load('stem_matrix')\n",
    "    if tf_idf:\n",
    "        data = TfidfTransformer(use_idf=use_idf, smooth_idf=smooth_idf, sublinear_tf=sublinear_tf).fit_transform(data)\n",
    "    if norm:\n",
    "        normalize(data, norm, copy=False)\n",
    "    if binary:\n",
    "        data.data[:] = 1\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(s):\n",
    "    # remove spaces if any\n",
    "    \n",
    "    if ' ' in s:\n",
    "        s = s.replace(' ', '')\n",
    "    return _parse(s).copy()\n",
    "\n",
    "def _apply(fun, items):\n",
    "    assert len(items) > 0\n",
    "    return reduce(fun, items[1:], items[0])\n",
    "\n",
    "def _parse(s):\n",
    "    text = load('text_features')\n",
    "    function = re.compile(r'^(\\w*)\\(([^)]*)\\)$')\n",
    "    \n",
    "    plus = lambda x, y: x + y\n",
    "    times = lambda x, y: x * y\n",
    "    \n",
    "    # replace some strings\n",
    "    if s == 'body':\n",
    "        s = 'h1+h2+h3+img+a+other'\n",
    "    elif s == 'other':\n",
    "        s = 'body'\n",
    "    \n",
    "    # apply functions\n",
    "    if function.match(s):\n",
    "        name, param = function.match(s).group(1, 2)\n",
    "        \n",
    "        if param == 'all':\n",
    "            param = ','.join(text)\n",
    "        \n",
    "        items = list(map(_parse, param.split(',')))\n",
    "        \n",
    "        return _apply({'max': maximum, 'sum': plus}[name], items)\n",
    "    \n",
    "    # addition and multiplication\n",
    "    if '+' in s:\n",
    "        items = list(map(_parse, s.split('+')))\n",
    "        return _apply(plus, items)\n",
    "    \n",
    "    if '*' in s:\n",
    "        items = list(map(_parse, s.split('*')))\n",
    "        return _apply(times, items)\n",
    "    \n",
    "    # try to parse any numbers\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # return corresponding dataset\n",
    "    return text[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse_maximum(A, B):\n",
    "    BisBigger = A-B\n",
    "    BisBigger.data = np.where(BisBigger.data < 0, 1, 0)\n",
    "    \n",
    "    return A - A.multiply(BisBigger) + B.multiply(BisBigger)\n",
    "\n",
    "def maximum(A, B):\n",
    "    from scipy.sparse import issparse, csr_matrix\n",
    "    \n",
    "    if issparse(A) or issparse(B):\n",
    "        return sparse_maximum(csr_matrix(A), csr_matrix(B))\n",
    "    else:\n",
    "        return np.maximum(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10566x175221 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 111256 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_featureset('max(h1, title)', tf_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
