{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Objective of the Notebook. **\n",
    "\n",
    "* Learn how to create new pipelines\n",
    "* Feature Engineering\n",
    "* Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import re, json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "basepath = os.path.expanduser('~/Desktop/src/Stumbleupon_classification_challenge/')\n",
    "sys.path.append(os.path.join(basepath, 'src'))\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from data import load_datasets\n",
    "from models import train_test_split, cross_val_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train, test, sample_sub = load_datasets.load_dataset()\n",
    "\n",
    "train_urlid = load_datasets.fetch_urlid(train)\n",
    "test_urlid = load_datasets.fetch_urlid(test)\n",
    "\n",
    "load_datasets.delete_urlid(train)\n",
    "load_datasets.delete_urlid(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['boilerplate'] = list(map(json.loads, train.boilerplate))\n",
    "test['boilerplate'] = list(map(json.loads, test.boilerplate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decompose_boilerplate(boilerplate_json, key='body'):\n",
    "    return [bp[key] if key in bp and bp[key] else u'' for bp in boilerplate_json]\n",
    "    \n",
    "train_body = decompose_boilerplate(train.boilerplate)\n",
    "train_title = decompose_boilerplate(train.boilerplate, key='title')\n",
    "\n",
    "test_body = decompose_boilerplate(test.boilerplate)\n",
    "test_title = decompose_boilerplate(test.boilerplate, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['body'] = train_body\n",
    "train['title'] = train_title\n",
    "\n",
    "test['body'] = test_body\n",
    "test['title'] = test_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_df = train[['body', 'title']]\n",
    "feature_df['label'] = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['body', 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 2,\n",
    "    'stratify': train.is_news\n",
    "}\n",
    "\n",
    "itrain, itest = train_test_split.tr_ts_split(len(train), **params)\n",
    "\n",
    "X_train = feature_df.iloc[itrain][features]\n",
    "X_test = feature_df.iloc[itest][features]\n",
    "\n",
    "y_train = feature_df.iloc[itrain].label\n",
    "y_test = feature_df.iloc[itest].label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task **\n",
    "\n",
    "* Text Preprocessing\n",
    "    * Lowercase all the string, remove stopwords, stem the words.\n",
    "* Decompose the boilerplate into body, title and url\n",
    "* Create text features for these parts and weigh them differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VarSelect(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(x):\n",
    "    return ' '.join([word for word in x.split(' ') if word not in ENGLISH_STOP_WORDS])\n",
    "\n",
    "def stem_tokens(x):\n",
    "    return ' '.join([sns.stem(word) for word in x.split(' ')])\n",
    "\n",
    "def preprocess_string(s):\n",
    "    s = s.lower()\n",
    "    stopwords_removed = remove_stopwords(s)\n",
    "    return stem_tokens(stopwords_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strip_non_words = FunctionTransformer(\n",
    "    lambda x: x.replace(r'^[A-Za-z0-9]+', ' ', regex=True), validate=False)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "            ('strip', strip_non_words),\n",
    "            ('union', FeatureUnion([\n",
    "                    ('lsa_body', Pipeline([\n",
    "                        ('var', VarSelect(keys='body')),\n",
    "                        ('tfidf', TfidfVectorizer(ngram_range=(1, 2), preprocessor=preprocess_string)),\n",
    "                        ('svd', TruncatedSVD(n_components=100))\n",
    "                    ])),\n",
    "                    ('lsa_title', Pipeline([\n",
    "                        ('var', VarSelect(keys='title')),\n",
    "                        ('tfidf', TfidfVectorizer(preprocessor=preprocess_string)),\n",
    "                        ('svd', TruncatedSVD(n_components=50))\n",
    "                    ])),\n",
    "                ])),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('feat', SelectKBest(f_classif, k=75)),\n",
    "            ('model', LogisticRegression())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('strip', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function <lambda> at 0x7f9789415268>, pass_y=False,\n",
       "          validate=False)), ('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('lsa_body', Pipeline(steps=[('var', VarSelect(keys='body')), ('tfidf', TfidfVectorizer(...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Private Leaderboard Score: 0.87247 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score on unseen examples:  0.860295516404\n"
     ]
    }
   ],
   "source": [
    "y_preds = pipeline.predict_proba(X_test)[:, 1]\n",
    "print('ROC AUC score on unseen examples: ', roc_auc_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = feature_df[features]\n",
    "y = feature_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('strip', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function <lambda> at 0x7f9789415268>, pass_y=False,\n",
       "          validate=False)), ('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('lsa_body', Pipeline(steps=[('var', VarSelect(keys='body')), ('tfidf', TfidfVectorizer(...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train on full dataset\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pipeline.predict_proba(test[['body', 'title']])[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_sub['label'] = predictions\n",
    "sample_sub.to_csv(os.path.join(basepath, 'submissions/ml_pipeline.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
