{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Objectives **\n",
    "* Learn how to parse html.\n",
    "* Create models that capture different aspects of the problem.\n",
    "* How to learn processes in parallel ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import re, json\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "basepath = os.path.expanduser('~/Desktop/src/Stumbleupon_classification_challenge/')\n",
    "sys.path.append(os.path.join(basepath, 'src'))\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "from data import load_datasets\n",
    "from models import train_test_split, cross_val_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stemmers\n",
    "sns = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, sample_sub = load_datasets.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['is_news'] = train.is_news.fillna(-999)\n",
    "test['is_news'] = test.is_news.fillna(-999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text Features based on the boiler plate\n",
    "* Text Features based on the parsed raw html\n",
    "* Numerical features\n",
    "* Train different models on different datasets and then use their predictions in the next stage of classifier and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_top_level_domain(url):\n",
    "        parsed_url = urlparse(url)\n",
    "        top_level = parsed_url[1].split('.')[-1]\n",
    "    \n",
    "        return top_level\n",
    "    \n",
    "def get_tlds(urls):\n",
    "    return np.array([extract_top_level_domain(url) for url in urls])\n",
    "\n",
    "train['tlds'] = get_tlds(train.url)\n",
    "test['tlds'] = get_tlds(test.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ohe = pd.get_dummies(list(train.tlds) + list(test.tlds))\n",
    "train = pd.concat((train, ohe.iloc[:len(train)]), axis=1)\n",
    "test = pd.concat((test, ohe.iloc[len(train):]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NumericalFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    @staticmethod\n",
    "    def url_depth(url):\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "\n",
    "        return len(list(filter(lambda x: len(x)> 0, path.split('/'))))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_url_depths(urls):\n",
    "        return np.array([NumericalFeatures.url_depth(url) for url in urls])\n",
    "    \n",
    "    def __init__(self, numerical_features):\n",
    "        self.features = numerical_features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df['url_depth'] = self.get_url_depths(df.url)\n",
    "        \n",
    "        numeric_features = self.features + ['url_depth']\n",
    "        df_numeric = df[numeric_features]\n",
    "        \n",
    "        return df_numeric\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split into training and test sets. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 2,\n",
    "    'stratify': train.is_news\n",
    "}\n",
    "\n",
    "itrain, itest = train_test_split.tr_ts_split(len(train), **params)\n",
    "\n",
    "X_train = train.iloc[itrain]\n",
    "X_test = train.iloc[itest]\n",
    "\n",
    "y_train = train.iloc[itrain].label\n",
    "y_test = train.iloc[itest].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_features = list(train.select_dtypes(exclude=['object']).columns[1:])\n",
    "numeric_features.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('feature_extractor', NumericalFeatures(numeric_features)),\n",
    "            ('imputer', Imputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', xgb.XGBClassifier(learning_rate=.08, max_depth=6))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_extractor', NumericalFeatures(numerical_features=None)), ('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', XGBClassifier(base_score=0.5, colsample_bylevel=1, cols...logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [ 0.76471212  0.73756162  0.76209055  0.73137451  0.72021488]\n",
      "Mean CV Score: 0.743191\n",
      "Std Cv Scoes: 0.017433\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "params = {\n",
    "    'n_folds': 5,\n",
    "    'shuffle': True,\n",
    "    'random_state': 3\n",
    "}\n",
    "\n",
    "scores, mean_score, std_score = cross_val_scheme.cv_scheme(pipeline, X_train, y_train, train.iloc[itrain].is_news, **params)\n",
    "\n",
    "print('CV Scores: %s'%(scores))\n",
    "print('Mean CV Score: %f'%(mean_score))\n",
    "print('Std Cv Scoes: %f'%(std_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score on the test set  0.753914951989\n"
     ]
    }
   ],
   "source": [
    "y_preds = pipeline.predict_proba(X_test)[:, 1]\n",
    "print('ROC AUC score on the test set ', roc_auc_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl_01.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl_02.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl_03.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl_04.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl_05.npy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline, os.path.join(basepath, 'data/processed/pipeline_numeric/pipeline_numeric.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Features based on the boiler plate. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = list(map(json.loads, train.boilerplate))\n",
    "test_json = list(map(json.loads, test.boilerplate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['boilerplate'] = train_json\n",
    "test['boilerplate'] = test_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component(boilerplate, key):\n",
    "    \"\"\"\n",
    "    Get value for a particular key in boilerplate json,\n",
    "    if present return the value else return an empty string\n",
    "    \n",
    "    boilerplate: list of boilerplate text in json format\n",
    "    key: key for which we want to fetch value e.g. body, title and url\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array([bp[key] if key in bp and bp[key] else u'' for bp in boilerplate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['body'] = get_component(train.boilerplate, 'body')\n",
    "test['body'] = get_component(test.boilerplate, 'body')\n",
    "\n",
    "train['title'] = get_component(train.boilerplate, 'title')\n",
    "test['title'] = get_component(test.boilerplate, 'title')\n",
    "\n",
    "train['url_component'] = get_component(train.boilerplate, 'url')\n",
    "test['url_component'] = get_component(test.boilerplate, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "class VarSelect(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.keys]\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.sns = sns\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        return [self.sns.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphanumeric(df):\n",
    "    return df.replace(r'[^A-Za-z0-9]+', ' ', regex=True)\n",
    "\n",
    "strip_non_words = FunctionTransformer(remove_non_alphanumeric, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemma Tokenizer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('strip', strip_non_words),\n",
    "    ('union', FeatureUnion([\n",
    "        ('body', Pipeline([\n",
    "            ('var', VarSelect(keys='body')),\n",
    "            ('tfidf', TfidfVectorizer(strip_accents='unicode', tokenizer=LemmaTokenizer(),\n",
    "                                     ngram_range=(1, 2), min_df=3, sublinear_tf=True)),\n",
    "            ('svd', TruncatedSVD(n_components=100))\n",
    "        ])),\n",
    "        ('title', Pipeline([\n",
    "            ('var', VarSelect(keys='title')),\n",
    "            ('tfidf', TfidfVectorizer(strip_accents='unicode', tokenizer=LemmaTokenizer(),\n",
    "                                     ngram_range=(1, 2), min_df=3, sublinear_tf=True)),\n",
    "            ('svd', TruncatedSVD(n_components=100))\n",
    "        ])),\n",
    "        ('url', Pipeline([\n",
    "            ('var', VarSelect(keys='url_component')),\n",
    "            ('tfidf', TfidfVectorizer(strip_accents='unicode', tokenizer=LemmaTokenizer(),\n",
    "                                     ngram_range=(1,2), min_df=3, sublinear_tf=True)),\n",
    "            ('svd', TruncatedSVD(n_components=50))\n",
    "        ]))\n",
    "    ])),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('selection', SelectKBest(chi2, k=100)),\n",
    "    ('model', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 2,\n",
    "    'stratify': train.is_news\n",
    "}\n",
    "\n",
    "itrain, itest = train_test_split.tr_ts_split(len(train), **params)\n",
    "\n",
    "features = ['url_component', 'body', 'title']\n",
    "\n",
    "X_train = train.iloc[itrain][features]\n",
    "X_test = train.iloc[itest][features]\n",
    "\n",
    "y_train = train.iloc[itrain].label\n",
    "y_test = train.iloc[itest].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('strip', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function remove_non_alphanumeric at 0x7f85f4503730>,\n",
       "          pass_y=False, validate=False)), ('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('body', Pipeline(steps=[('var', VarSelect(keys='body')), ('tfidf', Tfidf...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on unseen examples are:  0.868649291267\n"
     ]
    }
   ],
   "source": [
    "y_preds = pipeline.predict_proba(X_test)[:, 1]\n",
    "print('AUC score on unseen examples are: ', roc_auc_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_01.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_02.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_03.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_04.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_05.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_06.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_07.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_08.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_09.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_10.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_11.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_12.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_13.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_14.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_15.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_16.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_17.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_18.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_19.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_20.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_21.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_22.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_23.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_24.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_25.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_26.npy']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save this model to disk\n",
    "joblib.dump(pipeline, os.path.join(basepath, 'data/processed/pipeline_boilerplate_lemma/model_lemma.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(object):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "       \n",
    "    def fit(self, X, y=None):\n",
    "        cv = KFold(len(X), n_folds=3, shuffle=True, random_state=10)\n",
    "        model_perf = {}\n",
    "        \n",
    "        for model in models:\n",
    "            y_preds = np.array([])\n",
    "\n",
    "            for itrain, itest in cv:\n",
    "                Xtr = X.iloc[itrain]\n",
    "                ytr = y.iloc[itrain]\n",
    "\n",
    "                Xte = X.iloc[itest]\n",
    "                yte = y.iloc[itest]\n",
    "\n",
    "                y_preds = model.predict(Xte)[:, 1]\n",
    "            \n",
    "            model\n",
    "             \n",
    "        for model in self.models\n",
    "   \n",
    "    def transform(self, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
