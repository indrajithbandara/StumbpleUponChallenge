{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Objectives **\n",
    "* Learn how to parse html.\n",
    "* Create models that capture different aspects of the problem.\n",
    "* How to learn processes in parallel ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import re, json\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "basepath = os.path.expanduser('~/Desktop/src/Stumbleupon_classification_challenge/')\n",
    "sys.path.append(os.path.join(basepath, 'src'))\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "from data import load_datasets\n",
    "from models import train_test_split, cross_val_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, sample_sub = load_datasets.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['is_news'] = train.is_news.fillna(-999)\n",
    "test['is_news'] = test.is_news.fillna(-999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text Features based on the boiler plate\n",
    "* Text Features based on the parsed raw html\n",
    "* Numerical features\n",
    "* Train different models on different datasets and then use their predictions in the next stage of classifier and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_top_level_domain(url):\n",
    "        parsed_url = urlparse(url)\n",
    "        top_level = parsed_url[1].split('.')[-1]\n",
    "    \n",
    "        return top_level\n",
    "    \n",
    "def get_tlds(urls):\n",
    "    return np.array([extract_top_level_domain(url) for url in urls])\n",
    "\n",
    "train['tlds'] = get_tlds(train.url)\n",
    "test['tlds'] = get_tlds(test.url)\n",
    "\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(list(train['tlds']) + list(test['tlds']))\n",
    "\n",
    "train['tlds'] = lbl.transform(train['tlds'])\n",
    "test['tlds'] = lbl.transform(test['tlds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NumericalFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    @staticmethod\n",
    "    def url_depth(url):\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "\n",
    "        return len(list(filter(lambda x: len(x)> 0, path.split('/'))))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_url_depths(urls):\n",
    "        return np.array([NumericalFeatures.url_depth(url) for url in urls])\n",
    "    \n",
    "    def __init__(self, numerical_features):\n",
    "        self.features = numerical_features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df['url_depth'] = self.get_url_depths(df.url)\n",
    "        \n",
    "        numeric_features = self.features + ['url_depth']\n",
    "        df_numeric = df[numeric_features]\n",
    "        \n",
    "        return df_numeric\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split into training and test sets. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 2,\n",
    "    'stratify': train.is_news\n",
    "}\n",
    "\n",
    "itrain, itest = train_test_split.tr_ts_split(len(train), **params)\n",
    "\n",
    "X_train = train.iloc[itrain]\n",
    "X_test = train.iloc[itest]\n",
    "\n",
    "y_train = train.iloc[itrain].label\n",
    "y_test = train.iloc[itest].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'urlid', 'boilerplate', 'alchemy_category',\n",
       "       'alchemy_category_score', 'avglinksize', 'commonlinkratio_1',\n",
       "       'commonlinkratio_2', 'commonlinkratio_3', 'commonlinkratio_4',\n",
       "       'compression_ratio', 'embed_ratio', 'framebased', 'frameTagRatio',\n",
       "       'hasDomainLink', 'html_ratio', 'image_ratio', 'is_news',\n",
       "       'lengthyLinkDomain', 'linkwordscore', 'news_front_page',\n",
       "       'non_markup_alphanum_characters', 'numberOfLinks', 'numwords_in_url',\n",
       "       'parametrizedLinkRatio', 'spelling_errors_ratio', 'label', 'tlds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_features = list(train.select_dtypes(exclude=['object']).columns[1:])\n",
    "numeric_features.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('feature_extractor', NumericalFeatures(numeric_features)),\n",
    "            ('imputer', Imputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', LogisticRegression())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_extractor', NumericalFeatures(numerical_features=None)), ('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', LogisticRegression(C=1.0, class_weight=None, dual=False...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score on the test set  1.0\n"
     ]
    }
   ],
   "source": [
    "y_preds = pipeline.predict_proba(X_test)[:, 1]\n",
    "print('ROC AUC score on the test set ', roc_auc_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
