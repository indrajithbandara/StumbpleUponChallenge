{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Objectives **\n",
    "* Learn how to parse html.\n",
    "* Create models that capture different aspects of the problem.\n",
    "* How to learn processes in parallel ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import re, json\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "basepath = os.path.expanduser('~/Desktop/src/Stumbleupon_classification_challenge/')\n",
    "sys.path.append(os.path.join(basepath, 'src'))\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "from data import load_datasets\n",
    "from models import train_test_split, cross_val_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stemmers\n",
    "sns = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, sample_sub = load_datasets.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['is_news'] = train.is_news.fillna(-999)\n",
    "test['is_news'] = test.is_news.fillna(-999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text Features based on the boiler plate\n",
    "* Text Features based on the parsed raw html\n",
    "* Numerical features\n",
    "* Train different models on different datasets and then use their predictions in the next stage of classifier and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_top_level_domain(url):\n",
    "        parsed_url = urlparse(url)\n",
    "        top_level = parsed_url[1].split('.')[-1]\n",
    "    \n",
    "        return top_level\n",
    "    \n",
    "def get_tlds(urls):\n",
    "    return np.array([extract_top_level_domain(url) for url in urls])\n",
    "\n",
    "train['tlds'] = get_tlds(train.url)\n",
    "test['tlds'] = get_tlds(test.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ohe = pd.get_dummies(list(train.tlds) + list(test.tlds))\n",
    "train = pd.concat((train, ohe.iloc[:len(train)]), axis=1)\n",
    "test = pd.concat((test, ohe.iloc[len(train):]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NumericalFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    @staticmethod\n",
    "    def url_depth(url):\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "\n",
    "        return len(list(filter(lambda x: len(x)> 0, path.split('/'))))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_url_depths(urls):\n",
    "        return np.array([NumericalFeatures.url_depth(url) for url in urls])\n",
    "    \n",
    "    def __init__(self, numerical_features):\n",
    "        self.features = numerical_features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df['url_depth'] = self.get_url_depths(df.url)\n",
    "        \n",
    "        numeric_features = self.features + ['url_depth']\n",
    "        df_numeric = df[numeric_features]\n",
    "        \n",
    "        return df_numeric\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split into training and test sets. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 2,\n",
    "    'stratify': train.is_news\n",
    "}\n",
    "\n",
    "itrain, itest = train_test_split.tr_ts_split(len(train), **params)\n",
    "\n",
    "X_train = train.iloc[itrain]\n",
    "X_test = train.iloc[itest]\n",
    "\n",
    "y_train = train.iloc[itrain].label\n",
    "y_test = train.iloc[itest].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_features = list(train.select_dtypes(exclude=['object']).columns[1:])\n",
    "numeric_features.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('feature_extractor', NumericalFeatures(numeric_features)),\n",
    "            ('imputer', Imputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', xgb.XGBClassifier(learning_rate=.08, max_depth=6))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_extractor', NumericalFeatures(numerical_features=None)), ('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', XGBClassifier(base_score=0.5, colsample_bylevel=1, cols...logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [ 0.76471212  0.73756162  0.76209055  0.73137451  0.72021488]\n",
      "Mean CV Score: 0.743191\n",
      "Std Cv Scoes: 0.017433\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "params = {\n",
    "    'n_folds': 5,\n",
    "    'shuffle': True,\n",
    "    'random_state': 3\n",
    "}\n",
    "\n",
    "scores, mean_score, std_score = cross_val_scheme.cv_scheme(pipeline, X_train, y_train, train.iloc[itrain].is_news, **params)\n",
    "\n",
    "print('CV Scores: %s'%(scores))\n",
    "print('Mean CV Score: %f'%(mean_score))\n",
    "print('Std Cv Scoes: %f'%(std_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score on the test set  0.753914951989\n"
     ]
    }
   ],
   "source": [
    "y_preds = pipeline.predict_proba(X_test)[:, 1]\n",
    "print('ROC AUC score on the test set ', roc_auc_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl_01.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl_02.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl_03.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl_04.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_numeric/pipeline_numeric.pkl_05.npy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline, os.path.join(basepath, 'data/processed/pipeline_numeric/pipeline_numeric.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Features based on the boiler plate. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = list(map(json.loads, train.boilerplate))\n",
    "test_json = list(map(json.loads, test.boilerplate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['boilerplate'] = train_json\n",
    "test['boilerplate'] = test_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component(boilerplate, key):\n",
    "    \"\"\"\n",
    "    Get value for a particular key in boilerplate json,\n",
    "    if present return the value else return an empty string\n",
    "    \n",
    "    boilerplate: list of boilerplate text in json format\n",
    "    key: key for which we want to fetch value e.g. body, title and url\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array([bp[key] if key in bp and bp[key] else u'' for bp in boilerplate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['body'] = get_component(train.boilerplate, 'body')\n",
    "test['body'] = get_component(test.boilerplate, 'body')\n",
    "\n",
    "train['title'] = get_component(train.boilerplate, 'title')\n",
    "test['title'] = get_component(test.boilerplate, 'title')\n",
    "\n",
    "train['url_component'] = get_component(train.boilerplate, 'url')\n",
    "test['url_component'] = get_component(test.boilerplate, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "class VarSelect(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.keys]\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.sns = sns\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        return [self.sns.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphanumeric(df):\n",
    "    return df.replace(r'[^A-Za-z0-9]+', ' ', regex=True)\n",
    "\n",
    "strip_non_words = FunctionTransformer(remove_non_alphanumeric, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemma Tokenizer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('strip', strip_non_words),\n",
    "    ('union', FeatureUnion([\n",
    "        ('body', Pipeline([\n",
    "            ('var', VarSelect(keys='body')),\n",
    "            ('tfidf', TfidfVectorizer(strip_accents='unicode', tokenizer=LemmaTokenizer(),\n",
    "                                     ngram_range=(1, 2), min_df=3, sublinear_tf=True)),\n",
    "            ('svd', TruncatedSVD(n_components=100))\n",
    "        ])),\n",
    "        ('title', Pipeline([\n",
    "            ('var', VarSelect(keys='title')),\n",
    "            ('tfidf', TfidfVectorizer(strip_accents='unicode', tokenizer=LemmaTokenizer(),\n",
    "                                     ngram_range=(1, 2), min_df=3, sublinear_tf=True)),\n",
    "            ('svd', TruncatedSVD(n_components=100))\n",
    "        ])),\n",
    "        ('url', Pipeline([\n",
    "            ('var', VarSelect(keys='url_component')),\n",
    "            ('tfidf', TfidfVectorizer(strip_accents='unicode', tokenizer=LemmaTokenizer(),\n",
    "                                     ngram_range=(1,2), min_df=3, sublinear_tf=True)),\n",
    "            ('svd', TruncatedSVD(n_components=50))\n",
    "        ]))\n",
    "    ])),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('selection', SelectKBest(chi2, k=100)),\n",
    "    ('model', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 2,\n",
    "    'stratify': train.is_news\n",
    "}\n",
    "\n",
    "itrain, itest = train_test_split.tr_ts_split(len(train), **params)\n",
    "\n",
    "features = ['url_component', 'body', 'title']\n",
    "\n",
    "X_train = train.iloc[itrain][features]\n",
    "X_test = train.iloc[itest][features]\n",
    "\n",
    "y_train = train.iloc[itrain].label\n",
    "y_test = train.iloc[itest].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('strip', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function remove_non_alphanumeric at 0x7f85f4503730>,\n",
       "          pass_y=False, validate=False)), ('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('body', Pipeline(steps=[('var', VarSelect(keys='body')), ('tfidf', Tfidf...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on unseen examples are:  0.868649291267\n"
     ]
    }
   ],
   "source": [
    "y_preds = pipeline.predict_proba(X_test)[:, 1]\n",
    "print('AUC score on unseen examples are: ', roc_auc_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_01.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_02.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_03.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_04.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_05.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_06.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_07.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_08.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_09.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_10.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_11.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_12.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_13.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_14.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_15.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_16.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_17.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_18.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_19.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_20.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_21.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_22.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_23.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_24.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_25.npy',\n",
       " '/home/abhishek/Desktop/src/Stumbleupon_classification_challenge/data/processed/pipeline_boilerplate_lemma/model_lemma.pkl_26.npy']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save this model to disk\n",
    "joblib.dump(pipeline, os.path.join(basepath, 'data/processed/pipeline_boilerplate_lemma/model_lemma.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blending(object):\n",
    "    def __init__(self, models):\n",
    "        self.models = models # dict\n",
    "        \n",
    "    def predict(self, X, X_test, y=None):\n",
    "        cv = KFold(len(X), n_folds=3, shuffle=True, random_state=10)\n",
    "        \n",
    "        dataset_blend_train = np.zeros((X.shape[0], len(self.models.keys())))\n",
    "        dataset_blend_test = np.zeros((X_test.shape[0], len(self.models.keys())))\n",
    "        \n",
    "        for index, key in enumerate(self.models.keys()):\n",
    "            dataset_blend_test_index = np.zeros((X_test.shape[0], len(cv)))\n",
    "            \n",
    "            model = self.models[key][1]\n",
    "            feature_list = self.models[key][0]\n",
    "            \n",
    "            print('Training model of type: ', key)\n",
    "            \n",
    "            for i , (itrain, itest) in enumerate(cv):\n",
    "                Xtr = X.iloc[itrain][feature_list]\n",
    "                ytr = y.iloc[itrain]\n",
    "\n",
    "                Xte = X.iloc[itest][feature_list]\n",
    "                yte = y.iloc[itest]\n",
    "\n",
    "                y_preds = model.predict_proba(Xte)[:, 1]\n",
    "                \n",
    "                dataset_blend_train[itest, index] = y_preds\n",
    "                dataset_blend_test_index[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "            dataset_blend_test[:, index] = dataset_blend_test_index.mean(1)\n",
    "             \n",
    "        print('\\nBlending')\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(dataset_blend_train, y)\n",
    "        \n",
    "        y_submission = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    "        y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "        \n",
    "        return y_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_numeric = joblib.load(os.path.join(basepath, 'data/processed/pipeline_numeric/pipeline_numeric.pkl'))\n",
    "pipeline_boilerplate = joblib.load(os.path.join(basepath, 'data/processed/pipeline_boilerplate_lemma/model_lemma.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = list(train.select_dtypes(exclude=['object']).columns[1:]) + ['url']\n",
    "numeric_features.remove('label')\n",
    "\n",
    "boilerplate_features = ['body', 'title', 'url_component']\n",
    "\n",
    "models = {\n",
    "    'numeric': [numeric_features, pipeline_numeric],\n",
    "    'boilerplate': [boilerplate_features, pipeline_boilerplate]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 2,\n",
    "    'stratify': train.is_news\n",
    "}\n",
    "\n",
    "itrain, itest = train_test_split.tr_ts_split(len(train), **params)\n",
    "\n",
    "features = list(numeric_features) + list(boilerplate_features)\n",
    "\n",
    "X_train = train.iloc[itrain][features]\n",
    "X_test = train.iloc[itest][features]\n",
    "\n",
    "y_train = train.iloc[itrain].label\n",
    "y_test = train.iloc[itest].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model of type:  boilerplate\n",
      "Training model of type:  numeric\n",
      "\n",
      "Blending\n"
     ]
    }
   ],
   "source": [
    "blend = Blending(models)\n",
    "y_blend = blend.predict(X_train, X_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score after blending  0.838121627801\n"
     ]
    }
   ],
   "source": [
    "print('AUC score after blending ', roc_auc_score(y_test, y_blend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
